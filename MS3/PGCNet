import math
import numpy as np
import torch
import torchvision
import os
import torch.nn as nn
from torch.nn.functional import interpolate
import torch.nn.functional as F
from backbone.P2T.p2t import p2t_tiny
from thop import profile
import sys
from collections import OrderedDict
from MS3.audio_query import AttentionGenerator
import functools
class MLP(nn.Module):
    """ Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(nn.Conv2d(n, k, kernel_size=1, stride=1, padding=0)
                                    for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x
class Classifier_Module(nn.Module):
    def __init__(self, dilation_series, padding_series, NoLabels, input_channel):
        super(Classifier_Module, self).__init__()
        self.conv2d_list = nn.ModuleList()
        for dilation, padding in zip(dilation_series, padding_series):
            self.conv2d_list.append(
                nn.Conv2d(input_channel, NoLabels, kernel_size=3, stride=1, padding=padding, dilation=dilation,
                          bias=True))
        for m in self.conv2d_list:
            # 初始化权重操作，有什么用呢？
            # 让参数初始化为高斯分布（0,0.01）上的随机值
            m.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.conv2d_list[0](x)
        # print('out', out.shape)
        for i in range(len(self.conv2d_list) - 1):
            # print('self.conv2d_list[i+1](x)',self.conv2d_list[i+1](x).shape)
            out += self.conv2d_list[i + 1](x)
        return out
class ResidualConvUnit(nn.Module):
    """Residual convolution module.
    """

    def __init__(self, features):
        """Init.
        Args:
            features (int): number of features
        """
        super().__init__()

        self.conv1 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )
        self.conv2 = nn.Conv2d(
            features, features, kernel_size=3, stride=1, padding=1, bias=True
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        """Forward pass.
        Args:
            x (tensor): input
        Returns:
            tensor: output
        """
        # 这个地方采用的是两个3x3的卷积，跟使用一个5x5的卷积一个效果
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)

        return out + x
class FeatureFusionBlock(nn.Module):
    """Feature fusion block.
    """

    def __init__(self, features):
        """Init.
        Args:
            features (int): number of features
        """
        super(FeatureFusionBlock, self).__init__()

        self.resConfUnit1 = ResidualConvUnit(features)
        self.resConfUnit2 = ResidualConvUnit(features)

    def forward(self, *xs):
        """Forward pass.
        Returns:
            tensor: output
        """
        output = xs[0]
        # print('output', output.shape)
        if len(xs) == 2:
            output += self.resConfUnit1(xs[1])

        output = self.resConfUnit2(output)
        # print('output', output.shape)
        output = nn.functional.interpolate(
            output, scale_factor=2, mode="bilinear", align_corners=True
        )

        return output
class AUXFeatureFusionBlock(nn.Module):
    """Feature fusion block.
    """

    def __init__(self, features):
        """Init.
        Args:
            features (int): number of features
        """
        super(AUXFeatureFusionBlock, self).__init__()

        self.resConfUnit1 = ResidualConvUnit(features)
        self.resConfUnit2 = ResidualConvUnit(features)

    def forward(self, *xs):
        """Forward pass.
        Returns:
            tensor: output
        """
        output = xs[0]
        # print('output', output.shape)
        if len(xs) == 2:
            output += self.resConfUnit1(xs[1])

        output = self.resConfUnit2(output)
        # print('output', output.shape)
        output = nn.functional.interpolate(
            output, scale_factor=2, mode="bilinear", align_corners=True
        )

        return output
class Interpolate(nn.Module):
    """Interpolation module.
    """

    def __init__(self, scale_factor, mode, align_corners=False):
        """Init.
        Args:
            scale_factor (float): scaling
            mode (str): interpolation mode
        """
        super(Interpolate, self).__init__()

        self.interp = nn.functional.interpolate
        self.scale_factor = scale_factor
        self.mode = mode
        self.align_corners = align_corners

    def forward(self, x):
        """Forward pass.
        Args:
            x (tensor): input
        Returns:
            tensor: interpolated data
        """

        x = self.interp(
            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners
        )

        return x
class BysDn(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(BysDn, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, output_size)
        self.noise_mean = torch.nn.Parameter(torch.Tensor(1).uniform_(0.0, 0.1))
        self.noise_rho = torch.nn.Parameter(torch.Tensor(1).uniform_(-5, -4))
    def forward(self, x):
        noise_epsilon = torch.randn_like(x) * torch.exp(self.noise_rho) + self.noise_mean
        noisy_x = x + noise_epsilon
        out = torch.relu(self.fc1(noisy_x))
        out = self.fc2(out)
        return out
class BayesianLinear(torch.nn.Module):
    """
    Bayesian linear layer with normal prior and normal posterior
    """
    def __init__(self, in_features, out_features):
        super(BayesianLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight_mu = torch.nn.Parameter(torch.Tensor(out_features, in_features,1 ,1).uniform_(-0.2, 0.2))  # 权重的均值
        self.weight_rho = torch.nn.Parameter(torch.Tensor(out_features, in_features, 1, 1).uniform_(-5, -4)) # 权重的标准差
        self.bias_mu = torch.nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2)) # 偏置的均值
        self.bias_rho = torch.nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4)) # 偏置的标准差
        self.log_prior = 0.0
        self.log_variational_posterior = 0.0
    def forward(self, x):
        weight_epsilon = torch.randn(self.out_features, self.in_features, 1, 1).to(x.device)  #生成的权重正态系数
        bias_epsilon = torch.randn(self.out_features).to(x.device) #生成的偏置正太系数
        weight_std = torch.log1p(torch.exp(self.weight_rho))
        bias_std = torch.log1p(torch.exp(self.bias_rho))
        weight = self.weight_mu + weight_std * weight_epsilon
        bias = self.bias_mu + bias_std * bias_epsilon
        return F.conv2d(x, weight, bias)
class AIM(nn.Module):
    def __init__(self, dim=256, n_heads=8, qkv_bias=False, dropout=0.):
        super().__init__()
        self.dim = dim
        self.n_heads = n_heads
        self.dropout = dropout
        self.scale = (dim // n_heads)**-0.5
        self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv_proj = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(dropout)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(dropout)
    def forward(self, fv, fa):
        flatten_map = fv.flatten(2).transpose(1, 2)
        B, N, C = flatten_map.shape
        q = self.q_proj(fa).reshape(
            B, 1, self.n_heads, C // self.n_heads).permute(0, 2, 1, 3)
        kv = self.kv_proj(flatten_map).reshape(
            B, N, 2, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)
        k, v = kv.unbind(0)
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)
        x = self.proj_drop(self.proj(x))
        x = x.sigmoid()
        fusion_map = torch.einsum('bchw,bc->bchw', fv, x.squeeze())
        return fusion_map
class BasicConv2d(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride, padding, bias =False):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_planes, out_planes,
                              kernel_size=kernel_size, stride=stride,
                              padding=padding, bias = bias)
        self.bn = nn.BatchNorm2d(out_planes)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x
class WAM(nn.Module):
    def __init__(self, in_chanel):
        super(WAM, self).__init__()
        self.in_channel = in_chanel
        self.linearW = nn.Linear(in_chanel, in_chanel, bias=False)
    def forward(self, x1):
        size = x1.size()[2:]
        all_dim = size[0] * size[1]
        x1 = x1.view(-1, self.in_channel, all_dim)
        x11 = torch.transpose(x1, 1, 2).contiguous()
        x1_corr = self.linearW(x11)
        x111 = torch.bmm(x1, x1_corr)
        a1 = F.softmax(x111.clone(), dim=2)
        a1 = F.softmax(a1, dim=1)
        x1_out = torch.bmm(a1, x1).contiguous()
        x1_out = x1_out + x1
        out = x1_out.view(-1, self.in_channel, size[0], size[1])
        return out
class Pre(nn.Module):
    def __init__(self, in_planes, out_planes, reduction=16):
        super(Pre, self).__init__()
        self.wam = WAM(in_planes)
        self.conv1x1 = BasicConv2d(2 * in_planes, out_planes, 1, 1, 0)
        self.conv3x3 = BasicConv2d(in_planes, out_planes, 3, 1, 1)
    def forward(self, guidePath, mainPath):
        combined = torch.cat((guidePath, mainPath), dim=1)
        combined = self.conv1x1(combined)
        channel_weight = self.wam(combined)
        channel_weight = self.conv3x3(channel_weight)
        out = mainPath * channel_weight + mainPath
        return out
class PGIM(nn.Module):
    def __init__(self,in_planes, out_planes,reduction=16, bn_momentum=0.0003):
        super(PGIM, self).__init__()
        self.in_planes = in_planes
        self.cfp = Pre(in_planes, out_planes, reduction)
        self.gate = BasicConv2d(in_planes * 2, 1, 1, 1, 0, bias=True)
        self.softmax = nn.Softmax(dim=1)
    def forward(self, x, t):
        re_x = self.cfp(t, x)
        re_t = self.cfp(x, t)
        cat_f = torch.cat([re_x, re_t], dim=1)
        atten = self.gate(cat_f)
        atten = self.softmax(atten)
        merge_out = x * atten + t * atten
        return merge_out
class PGCNet(nn.Module):
    def __init__(self, channel=256, vis_dim=[48, 96, 240, 384], in_channels=[48, 96, 240, 384], valid_indices=[1, 2, 3]):
        super(PGCNet, self).__init__()
        self.backbone = p2t_tiny()
        path = "/home/xug/PycharmProjects/AVSBench/backbone/P2T/retinanet_p2t_t_fpn_1x_coco-1e0959bd.pth"
        sk = torch.load(path)['state_dict']
        new_state_dice = OrderedDict()
        for k, v in sk.items():
            name = k[9:]
            new_state_dice[name] = v
        self.backbone.load_state_dict(new_state_dice, strict=False)
        self.vis_dim = vis_dim
        self.conv4 = self._make_pred_layer(Classifier_Module, [3, 6, 12, 18], [3, 6, 12, 18], channel, self.vis_dim[3])
        self.conv3 = self._make_pred_layer(Classifier_Module, [3, 6, 12, 18], [3, 6, 12, 18], channel, self.vis_dim[2])
        self.conv2 = self._make_pred_layer(Classifier_Module, [3, 6, 12, 18], [3, 6, 12, 18], channel, self.vis_dim[1])
        self.conv1 = self._make_pred_layer(Classifier_Module, [3, 6, 12, 18], [3, 6, 12, 18], channel, self.vis_dim[0])
        self.path4 = FeatureFusionBlock(channel)
        self.path3 = FeatureFusionBlock(channel)
        self.path2 = FeatureFusionBlock(channel)
        self.path1 = FeatureFusionBlock(channel)
        self.auxpath4 = AUXFeatureFusionBlock(channel)
        self.auxpath3 = AUXFeatureFusionBlock(channel)
        self.auxpath2 = AUXFeatureFusionBlock(channel)
        self.auxpath1 = AUXFeatureFusionBlock(channel)
        self.audio_proj = nn.Linear(128, 256)
        self.mlp = MLP(4, 512, 256, 2)
        self.output_conv = nn.Sequential(
            # 最后的卷积输出 256 ->128 -> 32 -> 1
            nn.Conv2d(channel, 128, kernel_size=3, stride=1, padding=1),
            Interpolate(scale_factor=2, mode="bilinear"),
            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),
        )
        self.auxoutput_conv = nn.Sequential(
            # 最后的卷积输出 256 ->128 -> 32 -> 1
            nn.Conv2d(channel, 128, kernel_size=3, stride=1, padding=1),
            Interpolate(scale_factor=2, mode="bilinear"),
            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),
        )
        self.attn = AttentionGenerator(num_layers=1, query_num=4)
        self.learnable_query = nn.Embedding(4, 256)
        self.query = nn.Embedding(4, 256)
        self.Nbys = BysDn(128, 512, 256)
        self.BayesianLinear1 = BayesianLinear(64, 256)
        self.BayesianLinear2 = BayesianLinear(128, 256)
        self.BayesianLinear3 = BayesianLinear(320, 256)
        self.BayesianLinear4 = BayesianLinear(640, 256)
        self.aim = AIM()
        self.sig = nn.Sigmoid()
        # self.promptmixer = PromptMixer()
        self.pgim = PGIM(256, 256)
    def _make_pred_layer(self, block, dilation_series, padding_series, NoLabels, input_channel):
        return block(dilation_series, padding_series, NoLabels, input_channel)
    def forward(self, video, P2T_large, audio_feat):
        video = self.backbone(video)
        r1 = self.conv1(video[0])
        r2 = self.conv2(video[1])
        r3 = self.conv3(video[2])
        r4 = self.conv4(video[3])
        audio = self.Nbys(audio_feat)
        p1 = self.BayesianLinear1(P2T_large[0])  # BF x 256 x 56 x 56
        p2 = self.BayesianLinear2(P2T_large[1])  # BF x 256 x 28 x 28
        p3 = self.BayesianLinear3(P2T_large[2])  # BF x 256 x 14 x 14
        p4 = self.BayesianLinear4(P2T_large[3])  # BF x 256 x  7 x  7
        bs = audio.shape[0]
        audio_query = self.attn(audio) + self.learnable_query.weight[None, :, :].repeat(bs, 1, 1).contiguous()
        pred_feature1 = torch.einsum(
            'bqc,bchw->bqhw', audio_query, p1)
        # print('pred', pred_feature1.shape)
        pred_feature2 = torch.einsum(
            'bqc,bchw->bqhw', audio_query, p2)
        pred_feature3 = torch.einsum(
            'bqc,bchw->bqhw', audio_query, p3)
        pred_feature4 = torch.einsum(
            'bqc,bchw->bqhw', audio_query, p4)
        # print('pred_', pred_feature.shape)
        pred_feature1 = self.mlp(pred_feature1)
        pred_feature2 = self.mlp(pred_feature2)
        pred_feature3 = self.mlp(pred_feature3)
        pred_feature4 = self.mlp(pred_feature4)
        preAVprompt1 = p1 + pred_feature1
        preAVprompt2 = p2 + pred_feature2
        preAVprompt3 = p3 + pred_feature3
        preAVprompt4 = p4 + pred_feature4
        pa1 = self.aim(preAVprompt1, audio)
        pa2 = self.aim(preAVprompt2, audio)
        pa3 = self.aim(preAVprompt3, audio)
        pa4 = self.aim(preAVprompt4, audio)
        ff1 = self.aim(r1, audio)
        ff2 = self.aim(r2, audio)
        ff3 = self.aim(r3, audio)
        ff4 = self.aim(r4, audio)
        # 辅助label输出
        ac4 = self.auxpath4(pa4)  # BF x 256 x 14 x 14
        ac3 = self.auxpath3(ac4, pa3)  # BF x 256 x 28 x 28
        ac2 = self.auxpath2(ac3, pa2)  # BF x 256 x 56 x 56
        ac1 = self.auxpath1(ac2, pa1)
        aux_pred = self.auxoutput_conv(ac1)  # BF x 1 x 224 x 224
        fusion1 = self.pgim(ff1 , pa1)
        fusion2 = self.pgim(ff2 , pa2)
        fusion3 = self.pgim(ff3 , pa3)
        fusion4 = self.pgim(ff4 , pa4)
        conv4_feat = self.path4(fusion4)  # BF x 256 x 14 x 14
        conv43 = self.path3(conv4_feat, fusion3)  # BF x 256 x 28 x 28
        conv432 = self.path2(conv43, fusion2)  # BF x 256 x 56 x 56
        conv4321 = self.path1(conv432, fusion1)   # BF x 256 x 112 x 112
        pred = self.output_conv(conv4321)  # BF x 1 x 224 x 224
        return pred, aux_pred


