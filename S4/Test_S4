import os
import time
import torch
import argparse
import logging

from avs_s4.config import cfg
from avs_s4.dataloader import S4Dataset
from avs_s4.torchvggish import vggish
from torch.utils.data import Dataset
from avs_s4.utils import pyutils
from avs_s4.utils.utility import mask_iou, Eval_Fmeasure, save_mask
from avs_s4.utils.system import setup_logging
from backbone.P2T.p2t import p2t_large
from collections import OrderedDict
os.environ["CUDA_VISIBLE_DEVICES"] = '0'


class audio_extractor(torch.nn.Module):
    def __init__(self, cfg, device):
        super(audio_extractor, self).__init__()
        self.audio_backbone = vggish.VGGish(cfg, device)

    def forward(self, audio):
        audio_fea = self.audio_backbone(audio)
        return audio_fea
class p2t(torch.nn.Module):
    def __init__(self):
        super(p2t, self).__init__()
        self.p2t = p2t_large()
        path = "/home/xug/PycharmProjects/AVSBench/backbone/P2T/retinanet_p2t_l_fpn_1x_coco-d0ce637b.pth"
        sk = torch.load(path)['state_dict']
        new_state_dice = OrderedDict()
        for k, v in sk.items():
            # print(k)
            name = k[9:]
            new_state_dice[name] = v
        self.p2t.load_state_dict(new_state_dice, strict=False)
    def forward(self, img):
        img_feature = self.p2t(img)

        return img_feature

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--session_name", default="S4", type=str, help="the MS3 setting")
    parser.add_argument("--visual_backbone", default="pmavs_final", type=str,
                        help="use resnet50 or pvt-v2 or swin-transformer as the visual backbone")

    parser.add_argument("--test_batch_size", default=1, type=int)
    parser.add_argument("--max_epoches", default=15, type=int)
    parser.add_argument("--lr", default=0.0001, type=float)
    parser.add_argument("--num_workers", default=4, type=int)
    parser.add_argument("--wt_dec", default=5e-4, type=float)

    parser.add_argument("--tpavi_stages", default=[0, 1, 2, 3], nargs='+', type=int,
                        help='add non-local block in which stages: [0, 1, 2, 3]')
    parser.add_argument("--tpavi_vv_flag", action='store_true', default=False, help='visual-visual self-attention')
    parser.add_argument("--tpavi_va_flag", action='store_true', default=True, help='visual-audio cross-attention')

    parser.add_argument("--weights", default='/home/xug/PycharmProjects/AVSBench/avs_s4/PTH/pmavs_abl/S4_woAimCat/best_epochs/S47748889.pth',type=str)
    parser.add_argument("--save_pred_mask", action='store_true', default='/home/xug/PycharmProjects/AVSBench/avs_s4/PTH/pmavs_abl/S4_woAimCat/result/', help="save predited masks or not")
    parser.add_argument('--log_dir', default='./test_logs', type=str)

    args = parser.parse_args()

    if (args.visual_backbone).lower() == "resnet":
        print('==> Use ResNet50 as the visual backbone...')
    elif (args.visual_backbone).lower() == "pmavs_final":
        from avs_s4.Prompt_abl import pmAvs_woAimSum as AVSModel
        print('==> Use pmavs as the visual backbone...')
    else:
        raise NotImplementedError("only support the resnet50 and pvt-v2")

        # Log directory
    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir)
        # Logs
    prefix = args.session_name
    log_dir = os.path.join(args.log_dir, '{}'.format(time.strftime(prefix + '_%Y%m%d-%H%M%S')))
    log_dir_ss = os.path.join(args.log_dir, '{}'.format(time.strftime(prefix + '_%Y%m%d-%H%M%S')))
    args.log_dir = log_dir

    # Set logger
    log_path = os.path.join(log_dir, 'log')
    if not os.path.exists(log_path):
        os.makedirs(log_path, exist_ok=True)

    setup_logging(filename=os.path.join(log_path, 'log.txt'))
    logger = logging.getLogger(__name__)
    logger.info('==> Config: {}'.format(cfg))
    logger.info('==> Arguments: {}'.format(args))
    logger.info('==> Experiment: {}'.format(args.session_name))

    # Model
    model = AVSModel.Pred_endecoder()
    model.load_state_dict(torch.load(args.weights), strict=False)
    model.cuda()
    logger.info('Load trained model %s' % args.weights)
    # audio backbone
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    audio_backbone = audio_extractor(cfg, device)
    audio_backbone.cuda()
    audio_backbone.eval()
    P2T_backbone = p2t()
    P2T_backbone.cuda()
    P2T_backbone.eval()
    # Test data
    split = 'test'
    test_dataset = S4Dataset(split)
    test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                                  batch_size=args.test_batch_size,
                                                  shuffle=False,
                                                  num_workers=args.num_workers,
                                                  pin_memory=True)

    avg_meter_miou = pyutils.AverageMeter('miou')
    avg_meter_F = pyutils.AverageMeter('F_score')

    # Test
    model.eval()
    with torch.no_grad():
        for n_iter, batch_data in enumerate(test_dataloader):
            imgs, audio, mask, category_list, video_name_list = batch_data  # [bs, 5, 3, 224, 224], [bs, 5, 1, 96, 64], [bs, 1, 1, 224, 224]

            imgs = imgs.cuda()
            audio = audio.cuda()
            mask = mask.cuda()
            B, frame, C, H, W = imgs.shape
            imgs = imgs.view(B * frame, C, H, W)
            mask = mask.view(B * frame, H, W)
            audio = audio.view(-1, audio.shape[2], audio.shape[3], audio.shape[4])
            with torch.no_grad():
                audio_feature = audio_backbone(audio)
                imgs_resnet = P2T_backbone(imgs)

            output, _ = model(imgs, imgs_resnet, audio_feature)  # [5, 1, 224, 224] = [bs=1 * T=5, 1, 224, 224]
            if args.save_pred_mask:
                mask_save_path = os.path.join(args.save_pred_mask + log_dir_ss, 'pred_masks')
                save_mask(output.squeeze(1), mask_save_path, category_list, video_name_list)
            miou = mask_iou(output.squeeze(1), mask)
            avg_meter_miou.add({'miou': miou})
            F_score = Eval_Fmeasure(output.squeeze(1), mask, log_dir)
            avg_meter_F.add({'F_score': F_score})

        miou = (avg_meter_miou.pop('miou'))
        F_score = (avg_meter_F.pop('F_score'))
        logger.info('avs_test miou: {}, F_score: {}'.format(miou.item(), F_score))
