import os
import time
import random
import torch
import numpy as np
import argparse
import logging
import torch.utils.data as data
from avs_s4.config import cfg
from mmcv import Config
from avs_s4.dataloader import S4Dataset
from datetime import datetime
from avs_s4.torchvggish import vggish
from avs_s4.loss import IouSemanticAwareLoss
from collections import defaultdict
from avs_s4.utils import pyutils
from avs_s4.utils.loss_util import LossUtil
from avs_s4.utils.utility import mask_iou
from avs_s4.utils.system import setup_logging
from backbone.P2T.p2t import p2t_large
from collections import OrderedDict
l2 = torch.nn.MSELoss().cuda()
class audio_extractor(torch.nn.Module):
    def __init__(self, cfg, device):
        super(audio_extractor, self).__init__()
        self.audio_backbone = vggish.VGGish(cfg, device)
    def forward(self, audio):
        audio_fea = self.audio_backbone(audio)
        return audio_fea
class p2t(torch.nn.Module):
    def __init__(self):
        super(p2t, self).__init__()
        self.p2t = p2t_large()
        path = "/home/xug/PycharmProjects/AVSBench/backbone/P2T/retinanet_p2t_l_fpn_1x_coco-d0ce637b.pth"
        sk = torch.load(path)['state_dict']
        new_state_dice = OrderedDict()
        for k, v in sk.items():
            # print(k)
            name = k[9:]
            new_state_dice[name] = v
        self.p2t.load_state_dict(new_state_dice, strict=False)
    def forward(self, img):
        img_feature = self.p2t(img)
        return img_feature
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--session_name", default="S4", type=str,
                        help="使用MS3是对数据里的Multi-sources下的数据进行训练，是多声源数据，也就是，可能同时有多个物体发声")
    parser.add_argument("--visual_backbone", default="", type=str,
                        help="use resnet50 or pvt-v2 as the visual backbone")
    parser.add_argument("--train_batch_size", default=4, type=int)
    parser.add_argument("--val_batch_size", default=1, type=int)
    parser.add_argument("--max_epoches", default=15, type=int)
    parser.add_argument("--lr", default=0.0001, type=float)
    parser.add_argument("--num_workers", default=4, type=int)
    parser.add_argument("--wt_dec", default=5e-4, type=float)
    parser.add_argument('--masked_av_flag', action='store_true', default=True,
                        help='使用作者论文里说的loss： sa/masked_va loss')
    parser.add_argument("--lambda_1", default=0.5, type=float, help='均衡系数weight for balancing l4 loss')
    parser.add_argument("--masked_av_stages", default=[0, 1, 2, 3], nargs='+', type=int,
                        help='作者的设置compute sa/masked_va loss in which stages: [0, 1, 2, 3]')
    parser.add_argument('--threshold_flag', action='store_true', default=True,
                        help='whether thresholding the generated masks')
    parser.add_argument("--mask_pooling_type", default='avg', type=str, help='the manner to downsample predicted masks')
    parser.add_argument('--norm_fea_flag', action='store_true', default=True,
                        help='音频标准化normalize audio-visual features')
    parser.add_argument('--closer_flag', action='store_true', default=False, help='use closer loss for masked_va loss')
    parser.add_argument('--euclidean_flag', action='store_true', default=False,
                        help='use euclidean distance for masked_va loss')
    parser.add_argument('--kl_flag', action='store_true', default=True, help='KL散度 use kl loss for masked_va loss')
    parser.add_argument("--load_s4_params", action='store_true', default=False,
                        help='use S4 parameters for initilization')
    parser.add_argument("--trained_s4_model_path", type=str, default='', help='pretrained S4 model')
    parser.add_argument("--tpavi_stages", default=[0, 1, 2, 3], nargs='+', type=int,
                        help='tpavi模块 add tpavi block in which stages: [0, 1, 2, 3]')
    parser.add_argument("--tpavi_vv_flag", action='store_true', default=False, help='视觉自注意visual-visual self-attention')
    parser.add_argument("--tpavi_va_flag", action='store_true', default=True, help='视听交叉注意visual-audio cross-attention')
    parser.add_argument("--weights", type=str, default='', help='初始训练预训练模型，可以不写path of trained model')
    parser.add_argument('--cfgg', default='/home/xug/PycharmProjects/AVSBench/avs_s4/config.py', type=str, help='config file path')
    parser.add_argument('--log_dir', default='/home/xug/PycharmProjects/AVSBench/avs_s4/PTH', type=str)
    args = parser.parse_args()
    if (args.visual_backbone).lower() == "resnet":
    elif (args.visual_backbone).lower() == "":
        from S4 import pmAvs_mix as AVSModel
        print('==> Use pmavs_p2t as the visual backbone...')
    else:
        raise NotImplementedError("only support the resnet50 and pvt-v2")
    # Fix seed
    FixSeed = 123
    random.seed(FixSeed)
    np.random.seed(FixSeed)
    torch.manual_seed(FixSeed)
    torch.cuda.manual_seed(FixSeed)

    # Log directory
    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir, exist_ok=True)
    # Logs
    prefix = args.session_name
    pth = os.path.join(args.log_dir, args.visual_backbone)
    log_dir = os.path.join(pth, '{}'.format(time.strftime(prefix + '_%Y%m%d-%H%M%S')))
    args.log_dir = log_dir
    cfgg = Config.fromfile(args.cfgg)
    # Checkpoints directory
    checkpoint_dir = os.path.join(log_dir, 'best_epochs')
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir, exist_ok=True)
    args.checkpoint_dir = checkpoint_dir

    # Set logger
    log_path = os.path.join(log_dir, 'log')
    if not os.path.exists(log_path):
        os.makedirs(log_path, exist_ok=True)
    setup_logging(filename=os.path.join(log_path, 'log.txt'))
    logger = logging.getLogger(__name__)
    logger.info('==> Config: {}'.format(cfg))
    logger.info('==> Arguments: {}'.format(args))
    logger.info('==> Experiment: {}'.format(args.session_name))

    # Model
    model = AVSModel.Pred_endecoder()
    # model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()
    model = torch.nn.DataParallel(model).cuda()

    model.train()
    logger.info("==> Total params: %.2fM" % ( sum(p.numel() for p in model.parameters()) / 1e6))

    # video backbone
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    audio_backbone = audio_extractor(cfg, device)
    audio_backbone.cuda()
    audio_backbone.eval()
    P2T_backbone = p2t()
    P2T_backbone.cuda()
    P2T_backbone.eval()
    # Data
    train_dataset = S4Dataset('train')
    train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=args.train_batch_size,shuffle=True,num_workers=args.num_workers,pin_memory=True)
    max_step = (len(train_dataset) // args.train_batch_size) * args.max_epoches

    val_dataset = S4Dataset('val')
    val_dataloader =torch.utils.data.DataLoader(val_dataset,
                                    batch_size=args.val_batch_size,
                                    shuffle=False,
                                    num_workers=args.num_workers,
                                    pin_memory=True)
    total_step = len(train_dataloader)
    # Optimizer
    model_params = model.parameters()
    optimizer = torch.optim.Adam(model_params, args.lr)
    loss_util = LossUtil(**cfgg.losses)
    avg_meter_miou = pyutils.AverageMeter('miou')

    # Train
    metrics = defaultdict(float)
    torch.cuda.empty_cache()
    best_epoch = 0
    global_step = 0
    miou_list = []
    max_miou = 0
    for epoch in range(args.max_epoches):
        for n_iter, batch_data in enumerate(train_dataloader):

            imgs, audio, mask= batch_data # [bs, 5, 3, 224, 224], [bs, 5, 1, 96, 64], [bs, 5 or 1, 1, 224, 224]

            imgs = imgs.cuda()
            audio = audio.cuda()
            mask = mask.cuda()
            B, frame, C, H, W = imgs.shape
            imgs = imgs.view(B*frame, C, H, W)
            mask = mask.view(B, H, W)
            audio = audio.view(-1, audio.shape[2], audio.shape[3], audio.shape[4]) # [B*T, 1, 96, 64]
            with torch.no_grad():
                audio_feature = audio_backbone(audio) # [B*T, 128]
                imgs_P2T = P2T_backbone(imgs)

            output, aux_pred= model(imgs, imgs_P2T, audio_feature)
            loss, loss_dict = IouSemanticAwareLoss(output, mask.unsqueeze(1).unsqueeze(1), aux_pred, **cfgg.losses)
            loss_util.add_loss(loss, loss_dict)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            global_step += 1
            if (global_step - 1) % 50 == 0:
                train_log = 'Iter:%5d/%5d, %slr: %.6f' % (
                    global_step - 1, max_step, loss_util.pretty_out(), optimizer.param_groups[0]['lr'])
                logger.info(train_log)
        # Validation:
        model.eval()
        with torch.no_grad():
            for n_iter, batch_data in enumerate(val_dataloader):
                imgs, audio, mask, _, _ = batch_data # [bs, 5, 3, 224, 224], [bs, 5, 1, 96, 64], [bs, 5, 1, 224, 224]

                imgs = imgs.cuda()
                audio = audio.cuda()
                mask = mask.cuda()
                B, frame, C, H, W = imgs.shape
                imgs = imgs.view(B*frame, C, H, W)
                mask = mask.view(B*frame, H, W)
                audio = audio.view(-1, audio.shape[2], audio.shape[3], audio.shape[4])
                audio_feature = audio_backbone(audio)
                imgs_P2T = P2T_backbone(imgs)
                output, _, = model(imgs, imgs_P2T, audio_feature)
                miou = mask_iou(output.squeeze(1), mask)
                avg_meter_miou.add({'miou': miou})

            miou = (avg_meter_miou.pop('miou'))
            if miou > max_miou:
                model_save_path = os.path.join(checkpoint_dir, args.session_name)
                torch.save(model.module.state_dict(), model_save_path + 'best{}'.format(epoch)+'epoch.pth')
                best_epoch = epoch
                logger.info('save best model to %s'%model_save_path)

            miou_list.append(miou)
            max_miou = max(miou_list)

            val_log = 'Epoch: {}, Miou: {}, maxMiou: {}'.format(epoch, miou, max_miou)
            # print(val_log)
            logger.info(val_log)

        model.train()
    logger.info('best val Miou {} at peoch: {}'.format(max_miou, best_epoch))



